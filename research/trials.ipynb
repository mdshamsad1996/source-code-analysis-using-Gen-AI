{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3OUr-z2yvrxj"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai tiktoken chromadb langchain langchain-community GitPython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIwcpOYSwfnD",
        "outputId": "7a9cef06-41ec-44f3-955c-78aa8a2bf27e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.37.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting GitPython\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.111.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.30.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.3)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting orjson>=3.9.12 (from chromadb)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.23 (from langchain)\n",
            "  Downloading langchain_core-0.2.23-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.93-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.23->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.23.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Downloading openai-1.37.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.5.5-py3-none-any.whl (584 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.2.11-py3-none-any.whl (990 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.23-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.93-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.47b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=f3e2fa8a79f15acf7f853b48a074276024a5b07854ab5395df122fdc9f475e39\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, smmap, python-multipart, python-dotenv, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, mypy-extensions, marshmallow, jsonpointer, humanfriendly, httptools, h11, dnspython, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, jsonpatch, httpcore, gitdb, email_validator, coloredlogs, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, httpx, GitPython, dataclasses-json, opentelemetry-sdk, opentelemetry-instrumentation-asgi, openai, langchain-core, fastapi-cli, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, fastapi, langchain, chromadb, langchain-community\n",
            "Successfully installed GitPython-3.1.43 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 chroma-hnswlib-0.7.6 chromadb-0.5.5 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.14 dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.4 gitdb-4.0.11 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 humanfriendly-10.0 jsonpatch-1.33 jsonpointer-3.0.0 kubernetes-30.1.0 langchain-0.2.11 langchain-community-0.2.10 langchain-core-0.2.23 langchain-text-splitters-0.2.2 langsmith-0.1.93 marshmallow-3.21.3 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.18.1 openai-1.37.0 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-instrumentation-0.47b0 opentelemetry-instrumentation-asgi-0.47b0 opentelemetry-instrumentation-fastapi-0.47b0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-util-http-0.47b0 orjson-3.10.6 overrides-7.7.0 posthog-3.5.0 pypika-0.48.9 python-dotenv-1.0.1 python-multipart-0.0.9 smmap-5.0.1 starlette-0.37.2 tiktoken-0.7.0 typing-inspect-0.9.0 uvicorn-0.30.3 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from git import Repo\n",
        "from langchain.text_splitter import Language\n",
        "from langchain.document_loaders.generic import GenericLoader\n",
        "from langchain.document_loaders.parsers import LanguageParser\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "1Nt7fm_Awpo6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jrnsEcmGy5IU",
        "outputId": "5c43d4f0-ee9f-4b39-dec5-92304cb49aa9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir test-repo"
      ],
      "metadata": {
        "id": "w31ZJr1fz36a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path = \"test_repo/\"\n",
        "Repo.clone_from(\"https://github.com/entbappy/End-to-end-ML-Project-Implementation\",to_path=repo_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPySyeGp1O1Q",
        "outputId": "90fc7d12-a621-4a31-b912-5bbff653cdc7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<git.repo.base.Repo '/content/test_repo/.git'>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtnAJmNa6y4p",
        "outputId": "cd22c086-7af0-4339-c1cb-d3a4ec68ad81"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  test_repo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone Github repositories"
      ],
      "metadata": {
        "id": "KqfhnrnQ74DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path=\"test_repo/\"\n",
        "loader = GenericLoader.from_filesystem(repo_path+'/src/mlProject',\n",
        "                                       glob='**/*',\n",
        "                                       suffixes = [\".py\"],\n",
        "                                       parser=LanguageParser(language=Language.PYTHON,parser_threshold=500)\n",
        "                                       )"
      ],
      "metadata": {
        "id": "apUUOIRX1vJM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "LuAJ0HBA53Ox"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jroemy7S57um",
        "outputId": "96e4b6d8-abc4-4aea-b96c-fd44bb02a8e8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'test_repo/src/mlProject/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport logging\\n\\nlogging_str = \"[%(asctime)s: %(levelname)s: %(module)s: %(message)s]\"\\n\\nlog_dir = \"logs\"\\nlog_filepath = os.path.join(log_dir,\"running_logs.log\")\\nos.makedirs(log_dir, exist_ok=True)\\n\\n\\nlogging.basicConfig(\\n    level= logging.INFO,\\n    format= logging_str,\\n\\n    handlers=[\\n        logging.FileHandler(log_filepath),\\n        logging.StreamHandler(sys.stdout)\\n    ]\\n)\\n\\nlogger = logging.getLogger(\"mlProjectLogger\")'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/components/data_validation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom mlProject import logger\\nimport pandas as pd\\nfrom mlProject.entity.config_entity import DataValidationConfig\\n                                    \\n\\n\\n\\nclass DataValiadtion:\\n    def __init__(self, config: DataValidationConfig):\\n        self.config = config\\n\\n\\n    def validate_all_columns(self)-> bool:\\n        try:\\n            validation_status = None\\n\\n            data = pd.read_csv(self.config.unzip_data_dir)\\n            all_cols = list(data.columns)\\n\\n            all_schema = self.config.all_schema.keys()\\n\\n            \\n            for col in all_cols:\\n                if col not in all_schema:\\n                    validation_status = False\\n                    with open(self.config.STATUS_FILE, \\'w\\') as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n                else:\\n                    validation_status = True\\n                    with open(self.config.STATUS_FILE, \\'w\\') as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n\\n            return validation_status\\n        \\n        except Exception as e:\\n            raise e\\n\\n'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/components/data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport urllib.request as request\\nimport zipfile\\nfrom mlProject import logger\\nfrom mlProject.utils.common import get_size\\nfrom mlProject.entity.config_entity import DataIngestionConfig\\nfrom pathlib import Path\\n\\n\\nclass DataIngestion:\\n    def __init__(self, config: DataIngestionConfig):\\n        self.config = config\\n\\n    \\n    def download_file(self):\\n        if not os.path.exists(self.config.local_data_file):\\n            filename, headers = request.urlretrieve(\\n                url = self.config.source_URL,\\n                filename = self.config.local_data_file\\n            )\\n            logger.info(f\"{filename} download! with following info: \\\\n{headers}\")\\n        else:\\n            logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\")\\n\\n    \\n    def extract_zip_file(self):\\n        \"\"\"\\n        zip_file_path: str\\n        Extracts the zip file into the data directory\\n        Function returns None\\n        \"\"\"\\n        unzip_path = self.config.unzip_dir\\n        os.makedirs(unzip_path, exist_ok=True)\\n        with zipfile.ZipFile(self.config.local_data_file, \\'r\\') as zip_ref:\\n            zip_ref.extractall(unzip_path)\\n\\n'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/components/model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport pandas as pd\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\nfrom mlProject.utils.common import save_json\\nfrom urllib.parse import urlparse\\nimport numpy as np\\nimport joblib\\nfrom mlProject.entity.config_entity import ModelEvaluationConfig\\nfrom pathlib import Path\\n\\n\\nclass ModelEvaluation:\\n    def __init__(self, config: ModelEvaluationConfig):\\n        self.config = config\\n\\n    \\n    def eval_metrics(self,actual, pred):\\n        rmse = np.sqrt(mean_squared_error(actual, pred))\\n        mae = mean_absolute_error(actual, pred)\\n        r2 = r2_score(actual, pred)\\n        return rmse, mae, r2\\n    \\n\\n\\n    def save_results(self):\\n\\n        test_data = pd.read_csv(self.config.test_data_path)\\n        model = joblib.load(self.config.model_path)\\n\\n        test_x = test_data.drop([self.config.target_column], axis=1)\\n        test_y = test_data[[self.config.target_column]]\\n        \\n        predicted_qualities = model.predict(test_x)\\n\\n        (rmse, mae, r2) = self.eval_metrics(test_y, predicted_qualities)\\n        \\n        # Saving metrics as local\\n        scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\\n        save_json(path=Path(self.config.metric_file_name), data=scores)\\n\\n\\n\\n'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/components/data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom mlProject import logger\\nfrom sklearn.model_selection import train_test_split\\nimport pandas as pd\\nfrom mlProject.entity.config_entity import DataTransformationConfig\\n\\n\\nclass DataTransformation:\\n    def __init__(self, config: DataTransformationConfig):\\n        self.config = config\\n\\n    \\n    ## Note: You can add different data transformation techniques such as Scaler, PCA and all\\n    #You can perform all kinds of EDA in ML cycle here before passing this data to the model\\n\\n    # I am only adding train_test_spliting cz this data is already cleaned up\\n\\n\\n    def train_test_spliting(self):\\n        data = pd.read_csv(self.config.data_path)\\n\\n        # Split the data into training and test sets. (0.75, 0.25) split.\\n        train, test = train_test_split(data)\\n\\n        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"),index = False)\\n        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"),index = False)\\n\\n        logger.info(\"Splited data into training and test sets\")\\n        logger.info(train.shape)\\n        logger.info(test.shape)\\n\\n        print(train.shape)\\n        print(test.shape)\\n        '),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/components/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/components/model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='import pandas as pd\\nimport os\\nfrom mlProject import logger\\nfrom sklearn.linear_model import ElasticNet\\nimport joblib\\nfrom mlProject.entity.config_entity import ModelTrainerConfig\\n\\n\\n\\nclass ModelTrainer:\\n    def __init__(self, config: ModelTrainerConfig):\\n        self.config = config\\n\\n    \\n    def train(self):\\n        train_data = pd.read_csv(self.config.train_data_path)\\n        test_data = pd.read_csv(self.config.test_data_path)\\n\\n\\n        train_x = train_data.drop([self.config.target_column], axis=1)\\n        test_x = test_data.drop([self.config.target_column], axis=1)\\n        train_y = train_data[[self.config.target_column]]\\n        test_y = test_data[[self.config.target_column]]\\n\\n\\n        lr = ElasticNet(alpha=self.config.alpha, l1_ratio=self.config.l1_ratio, random_state=42)\\n        lr.fit(train_x, train_y)\\n\\n        joblib.dump(lr, os.path.join(self.config.root_dir, self.config.model_name))\\n\\n'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/entity/config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='from dataclasses import dataclass\\nfrom pathlib import Path\\n\\n\\n@dataclass(frozen=True)\\nclass DataIngestionConfig:\\n    root_dir: Path\\n    source_URL: str\\n    local_data_file: Path\\n    unzip_dir: Path\\n\\n\\n\\n@dataclass(frozen=True)\\nclass DataValidationConfig:\\n    root_dir: Path\\n    STATUS_FILE: str\\n    unzip_data_dir: Path\\n    all_schema: dict\\n\\n\\n\\n@dataclass(frozen=True)\\nclass DataTransformationConfig:\\n    root_dir: Path\\n    data_path: Path\\n\\n\\n\\n@dataclass(frozen=True)\\nclass ModelTrainerConfig:\\n    root_dir: Path\\n    train_data_path: Path\\n    test_data_path: Path\\n    model_name: str\\n    alpha: float\\n    l1_ratio: float\\n    target_column: str\\n\\n\\n@dataclass(frozen=True)\\nclass ModelEvaluationConfig:\\n    root_dir: Path\\n    test_data_path: Path\\n    model_path: Path\\n    all_params: dict\\n    metric_file_name: Path\\n    target_column: str'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/entity/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/config/configuration.py', 'language': <Language.PYTHON: 'python'>}, page_content='from mlProject.constants import *\\nfrom mlProject.utils.common import read_yaml, create_directories\\nfrom mlProject.entity.config_entity import (DataIngestionConfig,\\n                                            DataValidationConfig,\\n                                            DataTransformationConfig,\\n                                            ModelTrainerConfig,\\n                                            ModelEvaluationConfig)\\n\\nclass ConfigurationManager:\\n    def __init__(\\n        self,\\n        config_filepath = CONFIG_FILE_PATH,\\n        params_filepath = PARAMS_FILE_PATH,\\n        schema_filepath = SCHEMA_FILE_PATH):\\n\\n        self.config = read_yaml(config_filepath)\\n        self.params = read_yaml(params_filepath)\\n        self.schema = read_yaml(schema_filepath)\\n\\n        create_directories([self.config.artifacts_root])\\n        \\n\\n    def get_data_ingestion_config(self) -> DataIngestionConfig:\\n        config = self.config.data_ingestion\\n\\n        create_directories([config.root_dir])\\n\\n        data_ingestion_config = DataIngestionConfig(\\n            root_dir=config.root_dir,\\n            source_URL=config.source_URL,\\n            local_data_file=config.local_data_file,\\n            unzip_dir=config.unzip_dir \\n        )\\n\\n        return data_ingestion_config\\n    \\n\\n    \\n    def get_data_validation_config(self) -> DataValidationConfig:\\n        config = self.config.data_validation\\n        schema = self.schema.COLUMNS\\n\\n        create_directories([config.root_dir])\\n\\n        data_validation_config = DataValidationConfig(\\n            root_dir=config.root_dir,\\n            STATUS_FILE=config.STATUS_FILE,\\n            unzip_data_dir = config.unzip_data_dir,\\n            all_schema=schema,\\n        )\\n\\n        return data_validation_config\\n    \\n\\n    \\n    def get_data_transformation_config(self) -> DataTransformationConfig:\\n        config = self.config.data_transformation\\n\\n        create_directories([config.root_dir])\\n\\n        data_transformation_config = DataTransformationConfig(\\n            root_dir=config.root_dir,\\n            data_path=config.data_path,\\n        )\\n\\n        return data_transformation_config\\n    \\n\\n    def get_model_trainer_config(self) -> ModelTrainerConfig:\\n        config = self.config.model_trainer\\n        params = self.params.ElasticNet\\n        schema =  self.schema.TARGET_COLUMN\\n\\n        create_directories([config.root_dir])\\n\\n        model_trainer_config = ModelTrainerConfig(\\n            root_dir=config.root_dir,\\n            train_data_path = config.train_data_path,\\n            test_data_path = config.test_data_path,\\n            model_name = config.model_name,\\n            alpha = params.alpha,\\n            l1_ratio = params.l1_ratio,\\n            target_column = schema.name\\n            \\n        )\\n\\n        return model_trainer_config\\n    \\n\\n\\n    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\\n        config = self.config.model_evaluation\\n        params = self.params.ElasticNet\\n        schema =  self.schema.TARGET_COLUMN\\n\\n        create_directories([config.root_dir])\\n\\n        model_evaluation_config = ModelEvaluationConfig(\\n            root_dir=config.root_dir,\\n            test_data_path=config.test_data_path,\\n            model_path = config.model_path,\\n            all_params=params,\\n            metric_file_name = config.metric_file_name,\\n            target_column = schema.name\\n           \\n        )\\n\\n        return model_evaluation_config'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/config/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/pipeline/stage_03_data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_transformation import DataTransformation\\nfrom mlProject import logger\\nfrom pathlib import Path\\n\\n\\nSTAGE_NAME = \"Data Transformation stage\"\\n\\nclass DataTransformationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n\\n    def main(self):\\n        try:\\n            with open(Path(\"artifacts/data_validation/status.txt\"), \"r\") as f:\\n                status = f.read().split(\" \")[-1]\\n\\n            if status == \"True\":\\n                config = ConfigurationManager()\\n                data_transformation_config = config.get_data_transformation_config()\\n                data_transformation = DataTransformation(config=data_transformation_config)\\n                data_transformation.train_test_spliting()\\n\\n            else:\\n                raise Exception(\"You data schema is not valid\")\\n\\n        except Exception as e:\\n            print(e)\\n'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/pipeline/stage_01_data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_ingestion import DataIngestion\\nfrom mlProject import logger\\n\\nSTAGE_NAME = \"Data Ingestion stage\"\\n\\n\\nclass DataIngestionTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        data_ingestion_config = config.get_data_ingestion_config()\\n        data_ingestion = DataIngestion(config=data_ingestion_config)\\n        data_ingestion.download_file()\\n        data_ingestion.extract_zip_file()\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataIngestionTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/pipeline/stage_04_model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.model_trainer import ModelTrainer\\nfrom mlProject import logger\\n\\n\\n\\nSTAGE_NAME = \"Model Trainer stage\"\\n\\nclass ModelTrainerTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        model_trainer_config = config.get_model_trainer_config()\\n        model_trainer_config = ModelTrainer(config=model_trainer_config)\\n        model_trainer_config.train()\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = ModelTrainerTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/pipeline/stage_05_model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.model_evaluation import ModelEvaluation\\nfrom mlProject import logger\\n\\n\\nSTAGE_NAME = \"Model evaluation stage\"\\n\\nclass ModelEvaluationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        model_evaluation_config = config.get_model_evaluation_config()\\n        model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\\n        model_evaluation_config.save_results()\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = ModelEvaluationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/pipeline/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/pipeline/stage_02_data_validation.py', 'language': <Language.PYTHON: 'python'>}, page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_validation import DataValiadtion\\nfrom mlProject import logger\\n\\n\\nSTAGE_NAME = \"Data Validation stage\"\\n\\nclass DataValidationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        data_validation_config = config.get_data_validation_config()\\n        data_validation = DataValiadtion(config=data_validation_config)\\n        data_validation.validate_all_columns()\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataValidationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/pipeline/prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"import joblib \\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\n\\n\\nclass PredictionPipeline:\\n    def __init__(self):\\n        self.model = joblib.load(Path('artifacts/model_trainer/model.joblib'))\\n\\n    \\n    def predict(self, data):\\n        prediction = self.model.predict(data)\\n\\n        return prediction\"),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/constants/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='from pathlib import Path\\n\\nCONFIG_FILE_PATH = Path(\"config/config.yaml\")\\nPARAMS_FILE_PATH = Path(\"params.yaml\")\\nSCHEMA_FILE_PATH = Path(\"schema.yaml\")'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/utils/common.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom box.exceptions import BoxValueError\\nimport yaml\\nfrom mlProject import logger\\nimport json\\nimport joblib\\nfrom ensure import ensure_annotations\\nfrom box import ConfigBox\\nfrom pathlib import Path\\nfrom typing import Any\\n\\n\\n\\n@ensure_annotations\\ndef read_yaml(path_to_yaml: Path) -> ConfigBox:\\n    \"\"\"reads yaml file and returns\\n\\n    Args:\\n        path_to_yaml (str): path like input\\n\\n    Raises:\\n        ValueError: if yaml file is empty\\n        e: empty file\\n\\n    Returns:\\n        ConfigBox: ConfigBox type\\n    \"\"\"\\n    try:\\n        with open(path_to_yaml) as yaml_file:\\n            content = yaml.safe_load(yaml_file)\\n            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\\n            return ConfigBox(content)\\n    except BoxValueError:\\n        raise ValueError(\"yaml file is empty\")\\n    except Exception as e:\\n        raise e\\n    \\n\\n\\n@ensure_annotations\\ndef create_directories(path_to_directories: list, verbose=True):\\n    \"\"\"create list of directories\\n\\n    Args:\\n        path_to_directories (list): list of path of directories\\n        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\\n    \"\"\"\\n    for path in path_to_directories:\\n        os.makedirs(path, exist_ok=True)\\n        if verbose:\\n            logger.info(f\"created directory at: {path}\")\\n\\n\\n@ensure_annotations\\ndef save_json(path: Path, data: dict):\\n    \"\"\"save json data\\n\\n    Args:\\n        path (Path): path to json file\\n        data (dict): data to be saved in json file\\n    \"\"\"\\n    with open(path, \"w\") as f:\\n        json.dump(data, f, indent=4)\\n\\n    logger.info(f\"json file saved at: {path}\")\\n\\n\\n\\n\\n@ensure_annotations\\ndef load_json(path: Path) -> ConfigBox:\\n    \"\"\"load json files data\\n\\n    Args:\\n        path (Path): path to json file\\n\\n    Returns:\\n        ConfigBox: data as class attributes instead of dict\\n    \"\"\"\\n    with open(path) as f:\\n        content = json.load(f)\\n\\n    logger.info(f\"json file loaded succesfully from: {path}\")\\n    return ConfigBox(content)\\n\\n\\n@ensure_annotations\\ndef save_bin(data: Any, path: Path):\\n    \"\"\"save binary file\\n\\n    Args:\\n        data (Any): data to be saved as binary\\n        path (Path): path to binary file\\n    \"\"\"\\n    joblib.dump(value=data, filename=path)\\n    logger.info(f\"binary file saved at: {path}\")\\n\\n\\n@ensure_annotations\\ndef load_bin(path: Path) -> Any:\\n    \"\"\"load binary data\\n\\n    Args:\\n        path (Path): path to binary file\\n\\n    Returns:\\n        Any: object stored in the file\\n    \"\"\"\\n    data = joblib.load(path)\\n    logger.info(f\"binary file loaded from: {path}\")\\n    return data\\n\\n\\n\\n@ensure_annotations\\ndef get_size(path: Path) -> str:\\n    \"\"\"get size in KB\\n\\n    Args:\\n        path (Path): path of the file\\n\\n    Returns:\\n        str: size in KB\\n    \"\"\"\\n    size_in_kb = round(os.path.getsize(path)/1024)\\n    return f\"~ {size_in_kb} KB\"\\n\\n\\n\\n\\n'),\n",
              " Document(metadata={'source': 'test_repo/src/mlProject/utils/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ2WGDtI59XC",
        "outputId": "e6efd072-d5f4-4842-e30a-9419a589980d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'test_repo/src/mlProject/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport logging\\n\\nlogging_str = \"[%(asctime)s: %(levelname)s: %(module)s: %(message)s]\"\\n\\nlog_dir = \"logs\"\\nlog_filepath = os.path.join(log_dir,\"running_logs.log\")\\nos.makedirs(log_dir, exist_ok=True)\\n\\n\\nlogging.basicConfig(\\n    level= logging.INFO,\\n    format= logging_str,\\n\\n    handlers=[\\n        logging.FileHandler(log_filepath),\\n        logging.StreamHandler(sys.stdout)\\n    ]\\n)\\n\\nlogger = logging.getLogger(\"mlProjectLogger\")'), Document(metadata={'source': 'test_repo/src/mlProject/components/data_validation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom mlProject import logger\\nimport pandas as pd\\nfrom mlProject.entity.config_entity import DataValidationConfig\\n                                    \\n\\n\\n\\nclass DataValiadtion:\\n    def __init__(self, config: DataValidationConfig):\\n        self.config = config\\n\\n\\n    def validate_all_columns(self)-> bool:\\n        try:\\n            validation_status = None\\n\\n            data = pd.read_csv(self.config.unzip_data_dir)\\n            all_cols = list(data.columns)\\n\\n            all_schema = self.config.all_schema.keys()\\n\\n            \\n            for col in all_cols:\\n                if col not in all_schema:\\n                    validation_status = False\\n                    with open(self.config.STATUS_FILE, \\'w\\') as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n                else:\\n                    validation_status = True\\n                    with open(self.config.STATUS_FILE, \\'w\\') as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n\\n            return validation_status\\n        \\n        except Exception as e:\\n            raise e\\n\\n'), Document(metadata={'source': 'test_repo/src/mlProject/components/data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport urllib.request as request\\nimport zipfile\\nfrom mlProject import logger\\nfrom mlProject.utils.common import get_size\\nfrom mlProject.entity.config_entity import DataIngestionConfig\\nfrom pathlib import Path\\n\\n\\nclass DataIngestion:\\n    def __init__(self, config: DataIngestionConfig):\\n        self.config = config\\n\\n    \\n    def download_file(self):\\n        if not os.path.exists(self.config.local_data_file):\\n            filename, headers = request.urlretrieve(\\n                url = self.config.source_URL,\\n                filename = self.config.local_data_file\\n            )\\n            logger.info(f\"{filename} download! with following info: \\\\n{headers}\")\\n        else:\\n            logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\")\\n\\n    \\n    def extract_zip_file(self):\\n        \"\"\"\\n        zip_file_path: str\\n        Extracts the zip file into the data directory\\n        Function returns None\\n        \"\"\"\\n        unzip_path = self.config.unzip_dir\\n        os.makedirs(unzip_path, exist_ok=True)\\n        with zipfile.ZipFile(self.config.local_data_file, \\'r\\') as zip_ref:\\n            zip_ref.extractall(unzip_path)\\n\\n'), Document(metadata={'source': 'test_repo/src/mlProject/components/model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport pandas as pd\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\nfrom mlProject.utils.common import save_json\\nfrom urllib.parse import urlparse\\nimport numpy as np\\nimport joblib\\nfrom mlProject.entity.config_entity import ModelEvaluationConfig\\nfrom pathlib import Path\\n\\n\\nclass ModelEvaluation:\\n    def __init__(self, config: ModelEvaluationConfig):\\n        self.config = config\\n\\n    \\n    def eval_metrics(self,actual, pred):\\n        rmse = np.sqrt(mean_squared_error(actual, pred))\\n        mae = mean_absolute_error(actual, pred)\\n        r2 = r2_score(actual, pred)\\n        return rmse, mae, r2\\n    \\n\\n\\n    def save_results(self):\\n\\n        test_data = pd.read_csv(self.config.test_data_path)\\n        model = joblib.load(self.config.model_path)\\n\\n        test_x = test_data.drop([self.config.target_column], axis=1)\\n        test_y = test_data[[self.config.target_column]]\\n        \\n        predicted_qualities = model.predict(test_x)\\n\\n        (rmse, mae, r2) = self.eval_metrics(test_y, predicted_qualities)\\n        \\n        # Saving metrics as local\\n        scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\\n        save_json(path=Path(self.config.metric_file_name), data=scores)\\n\\n\\n\\n'), Document(metadata={'source': 'test_repo/src/mlProject/components/data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom mlProject import logger\\nfrom sklearn.model_selection import train_test_split\\nimport pandas as pd\\nfrom mlProject.entity.config_entity import DataTransformationConfig\\n\\n\\nclass DataTransformation:\\n    def __init__(self, config: DataTransformationConfig):\\n        self.config = config\\n\\n    \\n    ## Note: You can add different data transformation techniques such as Scaler, PCA and all\\n    #You can perform all kinds of EDA in ML cycle here before passing this data to the model\\n\\n    # I am only adding train_test_spliting cz this data is already cleaned up\\n\\n\\n    def train_test_spliting(self):\\n        data = pd.read_csv(self.config.data_path)\\n\\n        # Split the data into training and test sets. (0.75, 0.25) split.\\n        train, test = train_test_split(data)\\n\\n        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"),index = False)\\n        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"),index = False)\\n\\n        logger.info(\"Splited data into training and test sets\")\\n        logger.info(train.shape)\\n        logger.info(test.shape)\\n\\n        print(train.shape)\\n        print(test.shape)\\n        '), Document(metadata={'source': 'test_repo/src/mlProject/components/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''), Document(metadata={'source': 'test_repo/src/mlProject/components/model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='import pandas as pd\\nimport os\\nfrom mlProject import logger\\nfrom sklearn.linear_model import ElasticNet\\nimport joblib\\nfrom mlProject.entity.config_entity import ModelTrainerConfig\\n\\n\\n\\nclass ModelTrainer:\\n    def __init__(self, config: ModelTrainerConfig):\\n        self.config = config\\n\\n    \\n    def train(self):\\n        train_data = pd.read_csv(self.config.train_data_path)\\n        test_data = pd.read_csv(self.config.test_data_path)\\n\\n\\n        train_x = train_data.drop([self.config.target_column], axis=1)\\n        test_x = test_data.drop([self.config.target_column], axis=1)\\n        train_y = train_data[[self.config.target_column]]\\n        test_y = test_data[[self.config.target_column]]\\n\\n\\n        lr = ElasticNet(alpha=self.config.alpha, l1_ratio=self.config.l1_ratio, random_state=42)\\n        lr.fit(train_x, train_y)\\n\\n        joblib.dump(lr, os.path.join(self.config.root_dir, self.config.model_name))\\n\\n'), Document(metadata={'source': 'test_repo/src/mlProject/entity/config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='from dataclasses import dataclass\\nfrom pathlib import Path\\n\\n\\n@dataclass(frozen=True)\\nclass DataIngestionConfig:\\n    root_dir: Path\\n    source_URL: str\\n    local_data_file: Path\\n    unzip_dir: Path\\n\\n\\n\\n@dataclass(frozen=True)\\nclass DataValidationConfig:\\n    root_dir: Path\\n    STATUS_FILE: str\\n    unzip_data_dir: Path\\n    all_schema: dict\\n\\n\\n\\n@dataclass(frozen=True)\\nclass DataTransformationConfig:\\n    root_dir: Path\\n    data_path: Path\\n\\n\\n\\n@dataclass(frozen=True)\\nclass ModelTrainerConfig:\\n    root_dir: Path\\n    train_data_path: Path\\n    test_data_path: Path\\n    model_name: str\\n    alpha: float\\n    l1_ratio: float\\n    target_column: str\\n\\n\\n@dataclass(frozen=True)\\nclass ModelEvaluationConfig:\\n    root_dir: Path\\n    test_data_path: Path\\n    model_path: Path\\n    all_params: dict\\n    metric_file_name: Path\\n    target_column: str'), Document(metadata={'source': 'test_repo/src/mlProject/entity/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''), Document(metadata={'source': 'test_repo/src/mlProject/config/configuration.py', 'language': <Language.PYTHON: 'python'>}, page_content='from mlProject.constants import *\\nfrom mlProject.utils.common import read_yaml, create_directories\\nfrom mlProject.entity.config_entity import (DataIngestionConfig,\\n                                            DataValidationConfig,\\n                                            DataTransformationConfig,\\n                                            ModelTrainerConfig,\\n                                            ModelEvaluationConfig)\\n\\nclass ConfigurationManager:\\n    def __init__(\\n        self,\\n        config_filepath = CONFIG_FILE_PATH,\\n        params_filepath = PARAMS_FILE_PATH,\\n        schema_filepath = SCHEMA_FILE_PATH):\\n\\n        self.config = read_yaml(config_filepath)\\n        self.params = read_yaml(params_filepath)\\n        self.schema = read_yaml(schema_filepath)\\n\\n        create_directories([self.config.artifacts_root])\\n        \\n\\n    def get_data_ingestion_config(self) -> DataIngestionConfig:\\n        config = self.config.data_ingestion\\n\\n        create_directories([config.root_dir])\\n\\n        data_ingestion_config = DataIngestionConfig(\\n            root_dir=config.root_dir,\\n            source_URL=config.source_URL,\\n            local_data_file=config.local_data_file,\\n            unzip_dir=config.unzip_dir \\n        )\\n\\n        return data_ingestion_config\\n    \\n\\n    \\n    def get_data_validation_config(self) -> DataValidationConfig:\\n        config = self.config.data_validation\\n        schema = self.schema.COLUMNS\\n\\n        create_directories([config.root_dir])\\n\\n        data_validation_config = DataValidationConfig(\\n            root_dir=config.root_dir,\\n            STATUS_FILE=config.STATUS_FILE,\\n            unzip_data_dir = config.unzip_data_dir,\\n            all_schema=schema,\\n        )\\n\\n        return data_validation_config\\n    \\n\\n    \\n    def get_data_transformation_config(self) -> DataTransformationConfig:\\n        config = self.config.data_transformation\\n\\n        create_directories([config.root_dir])\\n\\n        data_transformation_config = DataTransformationConfig(\\n            root_dir=config.root_dir,\\n            data_path=config.data_path,\\n        )\\n\\n        return data_transformation_config\\n    \\n\\n    def get_model_trainer_config(self) -> ModelTrainerConfig:\\n        config = self.config.model_trainer\\n        params = self.params.ElasticNet\\n        schema =  self.schema.TARGET_COLUMN\\n\\n        create_directories([config.root_dir])\\n\\n        model_trainer_config = ModelTrainerConfig(\\n            root_dir=config.root_dir,\\n            train_data_path = config.train_data_path,\\n            test_data_path = config.test_data_path,\\n            model_name = config.model_name,\\n            alpha = params.alpha,\\n            l1_ratio = params.l1_ratio,\\n            target_column = schema.name\\n            \\n        )\\n\\n        return model_trainer_config\\n    \\n\\n\\n    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\\n        config = self.config.model_evaluation\\n        params = self.params.ElasticNet\\n        schema =  self.schema.TARGET_COLUMN\\n\\n        create_directories([config.root_dir])\\n\\n        model_evaluation_config = ModelEvaluationConfig(\\n            root_dir=config.root_dir,\\n            test_data_path=config.test_data_path,\\n            model_path = config.model_path,\\n            all_params=params,\\n            metric_file_name = config.metric_file_name,\\n            target_column = schema.name\\n           \\n        )\\n\\n        return model_evaluation_config'), Document(metadata={'source': 'test_repo/src/mlProject/config/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''), Document(metadata={'source': 'test_repo/src/mlProject/pipeline/stage_03_data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_transformation import DataTransformation\\nfrom mlProject import logger\\nfrom pathlib import Path\\n\\n\\nSTAGE_NAME = \"Data Transformation stage\"\\n\\nclass DataTransformationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n\\n    def main(self):\\n        try:\\n            with open(Path(\"artifacts/data_validation/status.txt\"), \"r\") as f:\\n                status = f.read().split(\" \")[-1]\\n\\n            if status == \"True\":\\n                config = ConfigurationManager()\\n                data_transformation_config = config.get_data_transformation_config()\\n                data_transformation = DataTransformation(config=data_transformation_config)\\n                data_transformation.train_test_spliting()\\n\\n            else:\\n                raise Exception(\"You data schema is not valid\")\\n\\n        except Exception as e:\\n            print(e)\\n'), Document(metadata={'source': 'test_repo/src/mlProject/pipeline/stage_01_data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_ingestion import DataIngestion\\nfrom mlProject import logger\\n\\nSTAGE_NAME = \"Data Ingestion stage\"\\n\\n\\nclass DataIngestionTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        data_ingestion_config = config.get_data_ingestion_config()\\n        data_ingestion = DataIngestion(config=data_ingestion_config)\\n        data_ingestion.download_file()\\n        data_ingestion.extract_zip_file()\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataIngestionTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n'), Document(metadata={'source': 'test_repo/src/mlProject/pipeline/stage_04_model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.model_trainer import ModelTrainer\\nfrom mlProject import logger\\n\\n\\n\\nSTAGE_NAME = \"Model Trainer stage\"\\n\\nclass ModelTrainerTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        model_trainer_config = config.get_model_trainer_config()\\n        model_trainer_config = ModelTrainer(config=model_trainer_config)\\n        model_trainer_config.train()\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = ModelTrainerTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n'), Document(metadata={'source': 'test_repo/src/mlProject/pipeline/stage_05_model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.model_evaluation import ModelEvaluation\\nfrom mlProject import logger\\n\\n\\nSTAGE_NAME = \"Model evaluation stage\"\\n\\nclass ModelEvaluationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        model_evaluation_config = config.get_model_evaluation_config()\\n        model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\\n        model_evaluation_config.save_results()\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = ModelEvaluationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n'), Document(metadata={'source': 'test_repo/src/mlProject/pipeline/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''), Document(metadata={'source': 'test_repo/src/mlProject/pipeline/stage_02_data_validation.py', 'language': <Language.PYTHON: 'python'>}, page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_validation import DataValiadtion\\nfrom mlProject import logger\\n\\n\\nSTAGE_NAME = \"Data Validation stage\"\\n\\nclass DataValidationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        data_validation_config = config.get_data_validation_config()\\n        data_validation = DataValiadtion(config=data_validation_config)\\n        data_validation.validate_all_columns()\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataValidationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n'), Document(metadata={'source': 'test_repo/src/mlProject/pipeline/prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"import joblib \\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\n\\n\\nclass PredictionPipeline:\\n    def __init__(self):\\n        self.model = joblib.load(Path('artifacts/model_trainer/model.joblib'))\\n\\n    \\n    def predict(self, data):\\n        prediction = self.model.predict(data)\\n\\n        return prediction\"), Document(metadata={'source': 'test_repo/src/mlProject/constants/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='from pathlib import Path\\n\\nCONFIG_FILE_PATH = Path(\"config/config.yaml\")\\nPARAMS_FILE_PATH = Path(\"params.yaml\")\\nSCHEMA_FILE_PATH = Path(\"schema.yaml\")'), Document(metadata={'source': 'test_repo/src/mlProject/utils/common.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom box.exceptions import BoxValueError\\nimport yaml\\nfrom mlProject import logger\\nimport json\\nimport joblib\\nfrom ensure import ensure_annotations\\nfrom box import ConfigBox\\nfrom pathlib import Path\\nfrom typing import Any\\n\\n\\n\\n@ensure_annotations\\ndef read_yaml(path_to_yaml: Path) -> ConfigBox:\\n    \"\"\"reads yaml file and returns\\n\\n    Args:\\n        path_to_yaml (str): path like input\\n\\n    Raises:\\n        ValueError: if yaml file is empty\\n        e: empty file\\n\\n    Returns:\\n        ConfigBox: ConfigBox type\\n    \"\"\"\\n    try:\\n        with open(path_to_yaml) as yaml_file:\\n            content = yaml.safe_load(yaml_file)\\n            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\\n            return ConfigBox(content)\\n    except BoxValueError:\\n        raise ValueError(\"yaml file is empty\")\\n    except Exception as e:\\n        raise e\\n    \\n\\n\\n@ensure_annotations\\ndef create_directories(path_to_directories: list, verbose=True):\\n    \"\"\"create list of directories\\n\\n    Args:\\n        path_to_directories (list): list of path of directories\\n        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\\n    \"\"\"\\n    for path in path_to_directories:\\n        os.makedirs(path, exist_ok=True)\\n        if verbose:\\n            logger.info(f\"created directory at: {path}\")\\n\\n\\n@ensure_annotations\\ndef save_json(path: Path, data: dict):\\n    \"\"\"save json data\\n\\n    Args:\\n        path (Path): path to json file\\n        data (dict): data to be saved in json file\\n    \"\"\"\\n    with open(path, \"w\") as f:\\n        json.dump(data, f, indent=4)\\n\\n    logger.info(f\"json file saved at: {path}\")\\n\\n\\n\\n\\n@ensure_annotations\\ndef load_json(path: Path) -> ConfigBox:\\n    \"\"\"load json files data\\n\\n    Args:\\n        path (Path): path to json file\\n\\n    Returns:\\n        ConfigBox: data as class attributes instead of dict\\n    \"\"\"\\n    with open(path) as f:\\n        content = json.load(f)\\n\\n    logger.info(f\"json file loaded succesfully from: {path}\")\\n    return ConfigBox(content)\\n\\n\\n@ensure_annotations\\ndef save_bin(data: Any, path: Path):\\n    \"\"\"save binary file\\n\\n    Args:\\n        data (Any): data to be saved as binary\\n        path (Path): path to binary file\\n    \"\"\"\\n    joblib.dump(value=data, filename=path)\\n    logger.info(f\"binary file saved at: {path}\")\\n\\n\\n@ensure_annotations\\ndef load_bin(path: Path) -> Any:\\n    \"\"\"load binary data\\n\\n    Args:\\n        path (Path): path to binary file\\n\\n    Returns:\\n        Any: object stored in the file\\n    \"\"\"\\n    data = joblib.load(path)\\n    logger.info(f\"binary file loaded from: {path}\")\\n    return data\\n\\n\\n\\n@ensure_annotations\\ndef get_size(path: Path) -> str:\\n    \"\"\"get size in KB\\n\\n    Args:\\n        path (Path): path of the file\\n\\n    Returns:\\n        str: size in KB\\n    \"\"\"\\n    size_in_kb = round(os.path.getsize(path)/1024)\\n    return f\"~ {size_in_kb} KB\"\\n\\n\\n\\n\\n'), Document(metadata={'source': 'test_repo/src/mlProject/utils/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunkings"
      ],
      "metadata": {
        "id": "_1JJIuUt77_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=2000, chunk_overlap=200)"
      ],
      "metadata": {
        "id": "4SldDvfS7k4H"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = documents_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "ooR1jmH38SfY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2gZZaWP8dlH",
        "outputId": "71777662-d3e7-45ec-bc23-103cc008557b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Model"
      ],
      "metadata": {
        "id": "dQRYG5-q9FL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "open_api_key = userdata.get('OPEN_API_KEY')"
      ],
      "metadata": {
        "id": "NmQPR-4V8fGH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = open_api_key"
      ],
      "metadata": {
        "id": "RtUW0qal9N4M"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(disallowed_special=())"
      ],
      "metadata": {
        "id": "r_y6AnaRCDpT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowledge base(vector database)"
      ],
      "metadata": {
        "id": "OMUl28hn99cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(text, embedding=embeddings, persist_directory=\"./data\")\n",
        "vectordb.persist()"
      ],
      "metadata": {
        "id": "Lkv1QPHw964A"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM Wrapper"
      ],
      "metadata": {
        "id": "LotVIWXNCnrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI()"
      ],
      "metadata": {
        "id": "VnWr-rY6ByNq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\",return_messages=True)"
      ],
      "metadata": {
        "id": "AQWl8ROsCtJ2"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectordb.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":3}), memory=memory)"
      ],
      "metadata": {
        "id": "fK_1cz6NDegi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q&A"
      ],
      "metadata": {
        "id": "WdBHrMJhKkMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"What is data ingestion?\"\n",
        "question =\"What is ModelTrianer class?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "AraQI9quKlo1"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36R7OYhlKtOa",
        "outputId": "b1024462-0a4d-4ce8-8bee-3775ce124cec"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 20 is greater than number of elements in index 19, updating n_results = 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXoN2fzBK5Kd",
        "outputId": "b64ac60f-5cfb-4e4b-8ddb-78a0d3a0345f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is ModelTrianer class?',\n",
              " 'chat_history': [SystemMessage(content='The human asks what data ingestion is. The AI explains that data ingestion is the process of collecting, importing, and processing data for storage or analysis, specifically in the context of downloading and extracting data files for machine learning training. In the context of machine learning training, data ingestion involves configuring setup, data ingestion, data extraction, data preparation, data storage, and training pipeline. The `DataIngestionTrainingPipeline` class handles the data ingestion process by downloading and extracting data files for training a machine learning model.')],\n",
              " 'answer': 'The `ModelTrainer` class is part of the ML project components and is responsible for training a machine learning model. It is initialized with a configuration object that contains parameters required for training the model. The class has a method `train()` that executes the training process.'}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drsnFcjwLBxa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}